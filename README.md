# [Emotional-computing-brain-computer-interface]

## 1.	Background
Emotional computing technology, which enables machines to understand human emotions, is becoming a research hotspot in the fields of human-computer interaction, mental health and artificial intelligence. Compared with speech, expression, heart rate and other behaviors and peripheral physiological signals, EEG can directly reflect individual emotional experience information, and EEG emotional computing/emotional brain-computer interface has been widely concerned by academic circles in recent years.
Due to the individual differences in emotional experience, how to realize cross-individual robust emotional recognition has always been a major challenge for the practical application of EEG emotional computing/emotional brain-computer interface (Hu et al. 2019). Facing this challenge, this contest provides participants with a batch of EEG data from 80 subjects with known emotional state information. Participants are required to establish an EEG calculation model with cross-individual emotional recognition ability, conduct real-time emotional recognition on another batch of subjects' EEG data, and determine the competition results according to the accuracy of emotional recognition.

### 2.Data and Evaluation
![image](https://user-images.githubusercontent.com/45135430/193395917-9a992233-ed33-454e-a59f-70113da454b1.png)
The EEG data acquisition process is shown in the above figure. Subjects watch a total of 28 emotional videos in 7 block. The 28 videos are composed of 4 negative emotions (anger, nausea, fear, sadness), 3 videos each, 4 videos of neutral emotions, and
Four kinds of positive emotions (pleasure, motivation, happiness and warmth) are composed of three videos. These video materials are taken from Chinese Emotional Fragments Standard Database (Ge et al., 2019), Positive Emotion Database (Hu et al., 2017, 2019) and FilmStim Database (Schaefer et al., 2010). The average length of video clips is 67s, ranging from 34s to 129s. Chinese subtitles have been re-added to the segments of the video that contain non-Chinese conversations to ensure that Chinese native speakers can fully understand the content of the video. For specific video information, please refer to Schedule 1- Emotional Inducing Materials.
Subjects watch 4 videos (positive, negative or neutral) with the same titer in each block, and watching one video is a trial. Each trial consists of five seconds of black screen gaze point presentation, video playback and emotional experience self-report. At the end of each trial, the subjects need to complete a simple emotional experience report, rest for at least 30 seconds, calm their emotions as much as possible and continue the experiment. At the end of each block, the subjects were asked to complete 20 math problems, so as to minimize the influence of previous emotional state on the follow-up experiments. See Li et al., 2020 and Hu et al., 2022 for more detailed information on data acquisition process.

